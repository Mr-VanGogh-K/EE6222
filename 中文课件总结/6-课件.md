---
title: 6
created: 2025-05-04
source: Cherry Studio
tags:
---

# MAP决策与分类器学习文档

## 一、引言：从直觉到数学理论的决策过程

决策理论是从人类直觉中提炼出的抽象数学理论，用于在不确定性环境下做出最优决策。PPT中通过一个简单例子介绍了这一过程：在一个电子电气工程学院（EEE）的教室里，你需要判断一个模糊可见的学生是男性还是女性。基于学院学生性别比例（男性70%，女性30%）的先验知识，你可能会判断该学生为男性。然而，如果场景切换到会计学院（男性30%，女性70%），你的决策可能会改变。这个直觉性的决策过程可以通过数学理论来科学化和系统化，即通过概率最大化来制定决策规则。

本章节将讨论如何从直觉决策过渡到最大后验概率（MAP）决策规则，以及如何基于数学模型构建分类器。具体内容包括：
- 最大后验概率（MAP）与贝叶斯决策规则；
- 模式识别系统的评估方法（如识别率和错误率）；
- 分类过程的泛化：判别函数；
- 多变量高斯分布下的判别函数；
- 马氏距离（Mahalanobis Distance）与欧几里得距离（Euclidean Distance）；
- 线性分类器及其特例。

---

## 二、决策规则的核心：最大后验概率（MAP）

### 2.1 先验概率与后验概率
在不确定性环境中，决策通常基于概率。以下是两个重要概念：
- **先验概率（Prior Probability）**：在没有观测数据时，基于背景知识对某类事件的概率估计。例如，EEE学院男性比例为$p(\omega_1) = 0.7$，女性比例为$p(\omega_2) = 0.3$。
- **后验概率（Posterior Probability）**：在观测到数据$x$后，对某类事件的概率更新。例如，知道学生身高为1.75米后，男性的后验概率表示为$p(\omega_1 | x)$，女性的后验概率为$p(\omega_2 | x)$。

### 2.2 MAP决策规则
MAP决策规则基于后验概率最大化，即选择后验概率最大的类别作为决策结果。数学表达为：
$$
\omega_k = \arg\max_{\omega_i} p(\omega_i | x)
$$
这意味着，如果$p(\omega_k | x) > p(\omega_i | x)$（对所有$i \neq k$），则决策为类别$\omega_k$。

MAP决策规则的优越性在于，它能够最小化错误决策的概率。错误概率定义为：
$$
p(e_k | x) = 1 - p(\omega_k | x)
$$
通过选择后验概率最大的类别，错误概率被最小化，因此MAP决策规则是最优的。

### 2.3 贝叶斯公式与类条件概率
直接计算后验概率$p(\omega_i | x)$通常较复杂，但可以通过贝叶斯公式将其转化为易于估计的先验概率和类条件概率：
$$
p(\omega_i | x) = \frac{p(\omega_i) p(x | \omega_i)}{p(x)}
$$
其中：
-$p(\omega_i)$是先验概率；
-$p(x | \omega_i)$是类条件概率（Class-Conditional Probability），表示在类别$\omega_i$下观测到数据$x$的概率；
-$p(x)$是混合概率密度（Mixture PDF），表示数据$x$出现的总概率，可通过全概率公式计算：
 $$
  p(x) = \sum_{i=1}^c p(x | \omega_i) p(\omega_i)
 $$
由于$p(x)$对所有类别相同，在决策中可以忽略，因此MAP决策规则可简化为：
$$
\omega_k = \arg\max_{\omega_i} [p(\omega_i) p(x | \omega_i)]
$$

---

## 三、模式识别系统的评估：错误率与识别率

### 3.1 单点错误率
对于某一特定观测值$x$，错误率由公式给出：
$$
p(e_k | x) = 1 - p(\omega_k | x)
$$
这表示在$x$处决策为$\omega_k$时，错误的可能性。

### 3.2 总体错误率
为了评估整个系统的性能，需要计算错误率在所有可能$x$上的期望值，即总体错误率：
- 对于离散型随机变量$x$：
 $$
  p(e) = \sum_{x=-\infty}^\infty p(e_k | x) p(x)
 $$
- 对于连续型随机变量$x$：
 $$
  p(e) = \int_{-\infty}^\infty p(e_k | x) p(x) \, dx
 $$

通过进一步推导，错误率可表达为：
$$
p(e) = 1 - \sum_{i=1}^c p(\omega_i) \int_{\Re_i} p(x | \omega_i) \, dx
$$
其中$\Re_i$是类别$\omega_i$的决策区域。正确识别率则为：
$$
p(\text{correct}) = 1 - p(e) = \sum_{i=1}^c p(\omega_i) \int_{\Re_i} p(x | \omega_i) \, dx
$$

### 3.3 二分类问题
对于二分类问题，错误率可进一步简化为：
$$
p(e) = p(\omega_1) \int_{\Re_2} p(x | \omega_1) \, dx + p(\omega_2) \int_{\Re_1} p(x | \omega_2) \, dx
$$
这表示错误率由两部分组成：类别1被错误分到类别2的概率，以及类别2被错误分到类别1的概率。

---

## 四、特征维度的扩展：从一维到多维

当观测数据从单一特征（如身高）扩展到多特征（如身高和头发长度），决策过程的原理不变，但数据表示变为向量形式：
$$
\mathbf{x} = [x_1, x_2]^T
$$
此时，概率分布从一维扩展到多维（如二维概率密度），决策边界从一维的阈值点变为二维平面上的曲线。增加特征维度通常能够提升正确决策的概率，降低错误率。

---

## 五、判别函数：分类过程的泛化

为了便于计算和实现自动化分类，可以将MAP决策规则转化为判别函数的形式。判别函数定义为：
$$
g_i(\mathbf{x}) = \ln p(\mathbf{x} | \omega_i) + \ln p(\omega_i)
$$
其中，对数函数$\ln$是一个单调递增函数，不会改变最大值的位置，但可以简化计算（尤其当概率密度为指数形式时）。

MAP决策规则等价于选择判别函数值最大的类别：
$$
\omega_k = \arg\max_{\omega_i} g_i(\mathbf{x})
$$

---

## 六、多变量高斯分布下的判别函数

假设类条件概率密度函数$p(\mathbf{x} | \omega_i)$服从多变量高斯分布：
$$
p(\mathbf{x} | \omega_i) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i)\right)
$$
其中：
-$\mu_i$是类别$\omega_i$的均值向量；
-$\Sigma_i$是类别$\omega_i$的协方差矩阵；
-$d$是特征维度。

代入判别函数公式，得到：
$$
g_i(\mathbf{x}) = -\frac{1}{2} (\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i) + \ln p(\omega_i) - \frac{1}{2} \ln |\Sigma_i| - \frac{d}{2} \ln 2\pi
$$
忽略与类别无关的常数项，简化为二次函数形式：
$$
g_i(\mathbf{x}) = -\frac{1}{2} (\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i) + b_i
$$
其中：
$$
b_i = \ln p(\omega_i) - \frac{1}{2} \ln |\Sigma_i|
$$

### 6.1 马氏距离与欧几里得距离
在判别函数中，项$(\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i)$称为马氏距离的平方：
$$
d_{\Sigma_i}(\mathbf{x}, \mu_i) = (\mathbf{x} - \mu_i)^T \Sigma_i^{-1} (\mathbf{x} - \mu_i)
$$
马氏距离考虑了数据的协方差结构，反映了数据分布的“形状”。相比之下，欧几里得距离的平方为：
$$
d_{Eu}(\mathbf{x}, \mu_i) = (\mathbf{x} - \mu_i)^T (\mathbf{x} - \mu_i)
$$
在一维情况下，马氏距离简化为：
$$
d_{\Sigma}(x, \mu_i) = \frac{(x - \mu_i)^2}{\sigma^2}
$$
欧几里得距离为：
$$
d_{Eu}(x, \mu_i) = (x - \mu_i)^2
$$
马氏距离通过协方差矩阵对数据进行标准化，能够更好地反映数据间的真实距离，尤其在多维情况下。

### 6.2 二次分类器
当协方差矩阵$\Sigma_i$不同时，判别函数为二次函数形式，决策边界为二次曲线（如抛物线、椭圆等）。这种分类器称为二次分类器。

---

## 七、线性分类器的特例

### 7.1 特例1：协方差矩阵相同
当所有类别的协方差矩阵相同（$\Sigma_i = \Sigma$）时，判别函数简化为线性形式：
$$
g_i(\mathbf{x}) = \mu_i^T \Sigma^{-1} \mathbf{x} - \frac{1}{2} \mu_i^T \Sigma^{-1} \mu_i + \ln p(\omega_i)
$$
记为：
$$
g_i(\mathbf{x}) = \mathbf{w}_i^T \mathbf{x} + w_{i0}
$$
其中$\mathbf{w}_i = \Sigma^{-1} \mu_i$，$w_{i0}$是偏置项。决策边界为线性超平面：
$$
g_i(\mathbf{x}) = g_j(\mathbf{x}) \implies (\mathbf{w}_i - \mathbf{w}_j)^T \mathbf{x} + w_{i0} - w_{j0} = 0
$$
此时分类器为线性分类器，决策边界通常不垂直于均值向量之间的连线（除非满足特定条件）。

### 7.2 特例2：协方差矩阵为对角矩阵且相同
当协方差矩阵为标量对角矩阵（$\Sigma_i = \sigma^2 \mathbf{I}$），表示特征间统计无关且方差相同，判别函数进一步简化为：
$$
g_i(\mathbf{x}) = \mu_i^T \mathbf{x} - \frac{1}{2} \mu_i^T \mu_i + \sigma^2 \ln p(\omega_i)
$$
决策边界为：
$$
(\mu_i - \mu_j)^T \mathbf{x} - \frac{1}{2} (\mu_i^T \mu_i - \mu_j^T \mu_j) + \sigma^2 \ln \left( \frac{p(\omega_i)}{p(\omega_j)} \right) = 0
$$
此时，决策超平面垂直于均值向量之间的连线。

### 7.3 特例3：先验概率相同且协方差矩阵为标量对角矩阵
当先验概率$p(\omega_i)$相同，且协方差矩阵为$\sigma^2 \mathbf{I}$，判别函数简化为欧几里得距离的形式：
$$
g_i(\mathbf{x}) = -\|\mathbf{x} - \mu_i\|^2
$$
此时，分类规则变为：将样本$\mathbf{x}$分配给距其最近的均值向量的类别。这种分类器称为**最小距离分类器**，本质上是一种模板匹配过程。

---

## 八、非高斯分布的处理

高斯分布是最常见且自然的分布形式，许多理论分类器基于此假设。对于非高斯分布，推导理论最优分类器非常困难甚至不可行。一种解决方法是通过特征变换将非高斯分布数据转化为近似高斯分布。例如，参考文献：
- J. Ren, X.D. Jiang, and J. Yuan, “A Chi-Squared-Transformed Subspace of LBP Histogram for Visual Recognition,” *IEEE Transactions on Image Processing*, vol. 24, no. 6, pp. 1893-1904, June 2015.
通过适当的变换方法，可以将复杂分布转换为高斯分布，从而应用上述分类器。

---

## 九、总结与扩展

本章节从直觉性决策出发，介绍了MAP决策规则和贝叶斯决策理论的核心思想。通过数学推导，将决策规则转化为判别函数的形式，并详细讨论了多变量高斯分布下的二次分类器和线性分类器的实现。同时，分析了马氏距离与欧几里得距离的区别，以及特征维度扩展对分类性能的影响。

### 学习要点
1. MAP决策规则通过最大化后验概率最小化错误率；
2. 判别函数是分类过程的泛化形式，便于计算和实现；
3. 高斯分布假设下，分类器可以是二次或线性的，具体取决于协方差矩阵的性质；
4. 增加特征维度通常提升分类性能，但也增加计算复杂度；
5. 非高斯分布可以通过特征变换处理。

