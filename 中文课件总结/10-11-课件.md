---
title: 10-11
created: 2025-05-04
source: Cherry Studio
tags:
---


# 神经网络与深度学习：从MLP到CNN再到Transformer

## 第一部分：神经网络基础与多层感知机（MLP）

### 1.1 神经网络架构与神经元模型
- **基本概念**：神经网络（Neural Network, NN）或人工神经网络（Artificial Neural Network, ANN）是一种模拟人脑神经元结构的计算模型。其基本单元是神经元，通过输入信号、权重、偏置和激活函数的组合，完成信息处理。
- **数学描述**：单个神经元的输出可以表示为：
 $$
  u_k = \sum_{j=1}^{p} w_{kj} x_j
 $$
 $$
  y_k = f(u_k - \theta_k)
 $$
  其中，$x_1, x_2, ..., x_p$是输入信号，$w_{k1}, w_{k2}, ..., w_{kp}$是神经元$k$的权重，$u_k$是线性组合输出，$\theta_k$是阈值，$f(\cdot)$是激活函数，$y_k$是神经元的输出。
- **阈值处理**：阈值$\theta_k$可被视为一个输入变量，通过引入$x_0 = 1$和$w_{k0} = -\theta_k$，公式简化为：
 $$
  v_k = \sum_{j=0}^{p} w_{kj} x_j
 $$
 $$
  y_k = f(v_k)
 $$
- **激活函数**：激活函数引入非线性特性，是神经网络实现复杂映射的关键。常见的激活函数包括：
  - **二值函数**：$f(v) = \begin{cases} 1 & v \geq 0 \\ 0 & v < 0 \end{cases}$
  - **分段线性函数**：$f(v) = \begin{cases} 1 & v \geq 0.5 \\ v & -0.5 < v < 0.5 \\ 0 & v \leq -0.5 \end{cases}$
  - **Sigmoid函数**：$f(v) = \frac{1}{1 + \exp(-a v)}$
  - **ReLU函数**：$f(v) = \max(0, v)$
  其中，ReLU（Rectified Linear Unit）因其简单性和梯度消失问题的缓解，在深度学习中被广泛使用。

### 1.2 多层感知机（MLP）架构
- **结构**：MLP由输入层、隐藏层和输出层组成。隐藏层中的神经元通过非线性激活函数处理输入，输出层根据任务类型（分类或回归）生成最终结果。
- **数学表达**：假设输入向量为$\mathbf{x} = [x_1, x_2, ..., x_n]^T$，隐藏层输出为$\mathbf{z} = [z_1, z_2, ..., z_d]^T$，最终输出为$\mathbf{y} = [y_1, y_2, ..., y_c]^T$，权重矩阵分别为$\mathbf{W}$和$\mathbf{V}$，则：
 $$
  z_j = f(\mathbf{w}_j^T \mathbf{x}) = f(\sum_{i=1}^{n} w_{ji} x_i)
 $$
 $$
  y_k = f(\mathbf{v}_k^T \mathbf{z}) = f(\sum_{j=1}^{d} v_{kj} z_j)
 $$
  矩阵形式为：
 $$
  \mathbf{y} = f(\mathbf{V}^T f(\mathbf{W}^T \mathbf{x}))
 $$
- **非线性重要性**：若激活函数$f$是线性的，则多层网络退化为单层网络，无法处理复杂非线性问题。因此，隐藏层必须使用非线性激活函数。

### 1.3 优化与迭代学习
- **目标**：通过调整权重$\mathbf{w}$和$\mathbf{v}$，最小化预测输出与真实输出之间的误差，通常使用均方误差（Mean Square Error, MSE）作为损失函数：
 $$
  J(\mathbf{w}) = E\{e^2\} = E\{(t - y)^2\} = E\{(t - \mathbf{w}^T \mathbf{x})^2\}
 $$
  其中，$e = t - y$是误差，$t$是目标输出。
- **单层网络优化**：对于单层网络，误差函数是二次函数，存在唯一的最小值点，可以通过解析解（最小二乘法）求得最优权重：
 $$
  \mathbf{w} = (\mathbf{X} \mathbf{X}^T)^{-1} \mathbf{X} \mathbf{t}^T
 $$
  其中，$\mathbf{X}$是输入矩阵，$\mathbf{t}$是目标向量。
- **梯度下降法**：也可以通过迭代更新权重，使用梯度下降法：
 $$
  \mathbf{w} \leftarrow \mathbf{w} + \eta e_i \mathbf{x}_i
 $$
  其中，$\eta$是学习率，$e_i = t_i - y_i$。

### 1.4 反向传播算法（Backpropagation）
- **目标**：对于多层网络，无法直接求解最优权重，需通过反向传播算法基于梯度下降迭代优化。
- **损失函数**：
 $$
  J(\mathbf{w}, \mathbf{v}) = \frac{1}{2} \sum_{k=1}^{c} (t_k - y_k)^2
 $$
- **权重更新**：
  - 输出层到隐藏层权重$\mathbf{v}_{kj}$：
   $$
    \frac{\partial J}{\partial v_{kj}} = -(t_k - y_k) f'(q_k) z_j
   $$
   $$
    v_{kj} \leftarrow v_{kj} + \eta (t_k - y_k) f'(q_k) z_j
   $$
  - 隐藏层到输入层权重$\mathbf{w}_{ji}$：
   $$
    \frac{\partial J}{\partial w_{ji}} = -\sum_{k=1}^{c} (t_k - y_k) f'(q_k) v_{kj} f'(q_j) x_i
   $$
   $$
    w_{ji} \leftarrow w_{ji} + \eta \sum_{k=1}^{c} v_{kj} (t_k - y_k) f'(q_k) f'(q_j) x_i
   $$
- **Sigmoid导数特性**：对于Sigmoid函数$f(q) = \frac{1}{1 + \exp(-a q)}$，其导数为：
 $$
  f'(q) = a f(q) (1 - f(q))
 $$
- **操作模式**：MLP有两种操作模式：
  - **前向传播（Feedforward）**：输入信号通过网络层级传递，计算输出。
  - **学习（Learning）**：通过监督学习调整权重，减少输出与目标的误差。
- **训练流程**：
  1. **初始化**：权重从均值为零的均匀分布中随机初始化。
  2. **训练样本呈现**：逐个输入训练样本，进行前向和反向计算。
  3. **前向计算**：计算各层激活值和输出。
  4. **反向计算**：计算局部梯度，更新权重。
  5. **迭代**：重复步骤直到停止条件（如误差收敛或迭代次数）满足。
- **问题与挑战**：
  - **局部极小值**：梯度下降可能陷入局部极小值，导致未找到全局最优解。
  - **过拟合与泛化问题**：训练集误差逐渐减小，但测试集误差可能增加，需通过验证集决定停止时机。

### 1.5 神经网络的结论
- **通用逼近性**：神经网络是通用逼近器，理论上两层MLP（使用Sigmoid激活函数）可任意逼近非线性连续函数，但实际中需要大量隐藏单元，且权重学习困难。
- **核心问题**：如何基于训练数据找到合适的非线性映射，以及解决局部极小值和过拟合问题是模式识别和机器学习的核心挑战。

## 第二部分：深度学习与卷积神经网络（CNN）

### 2.1 深度学习的动机
- **从MLP到深度学习**：理论上两层MLP即可逼近复杂非线性函数，但实际中优化困难。深度学习通过增加层数（$m \gg 2$）分解问题，逐步学习复杂的输入-输出映射。
- **过拟合问题**：机器学习的目标是学习通用规则，而非过拟合到训练数据。有限样本难以代表整个数据分布，需通过正则化限制模型复杂度。
- **正则化方法**：
  - 限制模型空间：$\min_{\hat{f} \in \Omega} \sum_i \| \mathbf{y}_i - \hat{f}(\mathbf{x}_i) \|_2^2$
  - 添加正则化项：$\min_{\hat{f}} \sum_i \| \mathbf{y}_i - \hat{f}(\mathbf{x}_i) \|_2^2 + \lambda \Phi(\hat{f})$

### 2.2 卷积神经网络（CNN）的特性
- **核心特性**：
  1. **网络架构**：CNN通过卷积操作处理输入数据（如图像），提取局部特征。
  2. **卷积（Convolution）**：通过滤波器提取局部信息，参数共享减少计算量。
  3. **非线性激活函数**：常用ReLU，简单且有效避免梯度消失。
  4. **池化（Pooling）**：降低空间维度，减少参数并增加鲁棒性。
  5. **深度结构**：多层网络逐步提取从低级到高级特征。
- **卷积操作**：输入特征图为$x_{i,j,k}$，输出特征图为$y_{i,j,k}$，卷积公式为：
 $$
  y_{i,j,k} = \sum_{n=1}^{c} \sum_{m=-1}^{1} \sum_{l=-1}^{1} w_{l,m,n,k} x_{i-l,j-m,n} + b_k
 $$
  其中，$w$是卷积核权重，$b_k$是偏置。
- **与MLP对比**：CNN是正则化后的MLP，通过权重共享和局部连接，避免过拟合。
- **小滤波器（3x3或1x1）的原因**：确保每个参数覆盖整个输入区域，学习通用规则而非特定区域特征。
- **深度网络的原因**：通过多层卷积逐步捕获大范围信息，ReLU激活函数支持深层学习。

### 2.3 CNN如何克服过拟合
- **参数共享**：每个学到的参数应用于整个输入区域（如图像的所有像素），避免特定区域过拟合。
- **逐步学习**：深度学习通过多层逐步提取特征，从低级边缘到高级语义，逐步逼近复杂的非线性关系。

### 2.4 CNN在图像处理中的应用
- **图像分割**：如像素级场景理解，采用上下文对比局部特征（Context Contrasted Local Features）和门控多尺度聚合（Gated Multi-scale Aggregation）等技术。
- **边界优化与形状变体卷积**：通过多尺度高斯扩散和语义相关性优化分割效果。

## 第三部分：从CNN到Transformer

### 3.1 Transformer简介
- **定义**：Transformer是一种神经网络架构，专门用于序列到序列的转换任务（如语音、文本、时间序列），最初为自然语言处理（NLP）设计，现广泛应用于计算机视觉，表现出优于CNN的能力。
- **优势**：相较于循环神经网络（RNN）和长短期记忆网络（LSTM），Transformer通过并行化处理序列操作提升效率。
- **核心模块**：注意力机制（Attention），捕获序列中每个词/ token与所有其他词的关系，核心论文为《Attention Is All You Need》。
- **问题与思考**：注意力是否是Transformer的全部？其与CNN的关系如何？Transformer是否仅因捕获全局信息而优于CNN？

### 3.2 Transformer架构
- **结构**：由编码器（Encoder）和解码器（Decoder）堆栈组成。
  - **编码器**：处理输入序列，生成中间表示。
  - **解码器**：基于编码器输出生成目标序列。
- **工作流程**：
  1. 将输入词/token嵌入特征向量。
  2. 通过多头注意力机制（Multi-head Attention）和前馈网络（Feed-forward Network）处理。
  3. 编码器和解码器均包含残差连接（Residual Connection）。
- **注意力机制**：
  - **自注意力（Self-Attention）**：序列内部的token相互关注，捕获内部关系。
  - **编码器-解码器注意力（Encoder-Decoder Attention）**：解码器中的token关注编码器输出。
  - **计算方式**：
    - 输入矩阵$\mathbf{X}$通过可学习线性投影生成查询（Query,$\mathbf{Q}$）、键（Key,$\mathbf{K}$）和值（Value,$\mathbf{V}$）：
     $$
      \mathbf{Q} = \mathbf{X} \mathbf{W}_q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_k, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_v
     $$
    - 注意力分数通过点积计算：
     $$
      \mathbf{W} = \mathbf{Q} \mathbf{K}^T
     $$
    - 输出为值的加权和：
     $$
      \mathbf{Z} = \mathbf{W} \mathbf{V}
     $$
- **特性**：注意力机制不可学习，权重由测试输入动态生成，捕获全局信息。

### 3.3 Transformer与CNN的关系
- **共同点**：Transformer的可学习模块（如线性投影和前馈网络）借鉴了CNN的卷积思想，参数共享，每个参数覆盖所有输入token。
- **差异**：
  - CNN通过卷积捕获局部信息，Transformer通过注意力捕获全局信息。
  - MLP虽也捕获全局信息，但易过拟合，Transformer通过动态注意力避免此问题。
- **视觉Transformer（ViT）**：在计算机视觉中，ViT在大数据预训练后优于CNN，尤其在需要全局依赖和上下文理解的任务中。

### 3.4 Transformer的应用
- **视觉-语言Transformer**：结合视觉和语言处理，如参考分割任务（Referring Segmentation）。
- **生成模型**：Transformer可用于生成任务，如机器翻译、文本生成等。

## 第四部分：深度学习的总结与展望
- **AI成功的关键**：在于机器学习，而机器学习的核心是学习通用规则而非过拟合训练数据。
- **CNN的成功**：通过最小滤波器大小的卷积操作实现参数共享，增强泛化能力；多层结构捕获大范围信息；ReLU激活函数支持深层学习。
- **Transformer的突破**：通过非可学习注意力模块捕获全局信息，避免过拟合，其可学习部分仍借鉴卷积思想。
- **未来方向**：结合CNN和Transformer优势，探索更高效的模型架构，进一步提升泛化能力和计算效率。
