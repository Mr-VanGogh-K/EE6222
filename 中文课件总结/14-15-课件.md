---
title: 14-15
created: 2025-05-04
source: Cherry Studio
tags:
---

---

# EE6222 Machine Vision - 3D 学习文档

## 第一部分：3D视觉概述

### 1.1 3D视觉的意义
3D视觉是机器视觉领域的重要分支，旨在从2D图像或传感器数据中恢复三维场景结构和相机运动。3D视觉技术广泛应用于机器人、自动驾驶、增强现实（AR）、医疗影像处理等领域。通过不同的方法和技术，可以从不同的数据源（如图像、视频、点云等）中提取3D信息。

### 1.2 3D视觉的主要方法
以下是实现3D视觉的几种主要技术，每种方法有其独特的原理和应用场景：
- **飞行时间（Time of Flight, ToF）**：利用光脉冲从发射到返回的时间差计算距离，常见于深度摄像头。
- **激光雷达（LiDAR, Light Detection and Ranging）**：一种基于ToF原理的传感器，通过激光束扫描环境，生成高精度的3D点云。
- **结构光（Structured Light）**：通过投影已知的光图案（如网格或条纹）到场景上，利用相机捕捉变形图案来推算深度。
- **运动结构恢复（Structure from Motion, SfM）**：从多幅图像中恢复场景的3D结构和相机运动。
- **立体视觉（Stereo Vision）**：利用两台相机的视差信息计算深度。

---

## 第二部分：3D视觉技术详解

### 2.1 飞行时间（Time of Flight, ToF）
- **原理**：ToF通过测量光从发射器到目标物体并返回传感器所需的时间，计算目标到传感器的距离。距离公式为$d = \frac{c \cdot t}{2}$，其中$c$为光速，$t$为往返时间。
- **应用**：常见于消费级深度摄像头（如微软Kinect）、工业测距。
- **局限性**：
  - 受背景光干扰严重，强光环境下精度下降。
  - 多重反射会导致测量误差。
  - 设备间相互干扰问题。
- **补充知识**：ToF相机通常使用红外光以避免可见光干扰，但其分辨率和范围受限于硬件成本。

### 2.2 激光雷达（LiDAR）
- **原理**：LiDAR是ToF技术的扩展，通过发射激光束并测量反射时间，构建3D点云数据。
- **应用**：广泛用于自动驾驶（环境感知）、机器人导航和地理测绘。
- **挑战**：
  - **LiDAR Ghost**：由于激光反射或散射导致的虚假读数或伪影。
  - **LiDAR Blooming**：当激光束被高反光物体（如汽车光泽漆面）反射回传感器时，会导致过饱和现象。
- **补充知识**：现代LiDAR系统（如Velodyne、Waymo）结合多束激光和旋转机制，实现360度高精度扫描。

### 2.3 结构光（Structured Light）
- **原理**：通过投影已知图案（如网格或条纹）到场景中，利用相机捕捉图案变形，计算深度信息。光源和相机之间的三角关系是计算深度基础。
- **应用**：常见于工业检测、3D扫描（如Face ID）。
- **局限性**：
  - 不适合户外环境，强光会干扰图案识别。
  - 对复杂表面的适应性较差。
- **补充知识**：结构光系统通常使用红外光图案，以减少可见光干扰。

### 2.4 运动结构恢复（Structure from Motion, SfM）
- **基本概念**：SfM从多幅图像（通常来自视频或不同视角）中恢复3D场景结构和相机运动轨迹。
- **核心步骤**：
  1. **关键点检测与跟踪**：在图像中找到对应点（特征点）。
  2. **相机参数估计**：计算相机位置和方向。
  3. **3D点重建**：利用对应点和相机参数重建3D结构。
- **投影模型**：
  - **透视投影（Perspective Projection）**：基于透视成像模型，3D点通过相机镜头投影到2D图像平面上。公式为：$x = f \cdot \frac{X}{Z}, y = f \cdot \frac{Y}{Z}$，其中$f$为焦距，$(X, Y, Z)$为3D坐标，$(x, y)$为图像坐标。
  - **仿射投影（Affine Projection）**：简化模型，适用于相机中心在无穷远的情况。
- **挑战**：
  - 需要准确的特征点对应。
  - 图像噪声和遮挡会影响重建质量。

#### 2.4.1 相机模型与欧拉角
- **相机模型**：相机通过旋转（R）和平移（t）将3D世界坐标投影到2D图像平面。投影包括透视变换。
- **欧拉角（Euler Angles）**：描述相机或物体的3D旋转，分为：
  - **Roll**：绕Z轴旋转。
  - **Pitch**：绕X轴旋转。
  - **Yaw**：绕Y轴旋转。
- **旋转矩阵**：由欧拉角转换得到，用于描述3D空间中的旋转。旋转矩阵公式为：
 $$
  R = R_z(\text{roll}) \cdot R_x(\text{pitch}) \cdot R_y(\text{yaw})
 $$
  其中每个矩阵的具体形式为：
  - 绕X轴旋转：$R_x(\theta) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta \end{bmatrix}$
  - 类似地可定义绕Y轴和Z轴的旋转矩阵。

#### 2.4.2 关键点检测与跟踪
- **关键点（Keypoint）定义**：图像中的显著点，如角点、纹理区域，具有不变性（如尺度、旋转）。
- **常见方法**：
  - **Harris角点检测**：通过计算图像梯度变化，识别角点位置。
  - **SIFT（Scale-Invariant Feature Transform）**：通过尺度空间极值检测关键点，并生成128维描述符，用于匹配。
  - **SuperPoint**：基于深度学习的特征点检测与描述方法，适用于实时应用。
- **跟踪方法**：
  - **Lucas-Kanade光流法**：假设亮度不变和小运动，计算关键点在连续帧间的位移。
  - **挑战**：关键点可能因旋转、阴影或遮挡而丢失，需要动态添加/删除关键点。

#### 2.4.3 特征匹配
- **目标**：在两幅或多幅图像中找到对应关键点。
- **方法**：
  - **个体关键点匹配**：基于描述符距离（如SIFT距离）找到最相似点，拒绝模糊匹配。
  - **全局匹配**：使用RANSAC（随机采样一致性）寻找最佳单应性矩阵（Homography），剔除外点。
- **现代方法**：
  - **SuperGlue**：基于图神经网络的特征匹配方法，考虑关键点间的全局上下文。
  - **LoFTR**：无检测器的局部特征匹配方法，直接在粗粒度级别进行像素级密集匹配。

#### 2.4.4 仿射结构恢复
- **仿射投影**：线性映射加平移，简化3D到2D的投影过程。
- **测量矩阵分解**：通过分解2D点坐标矩阵（秩为3）恢复相机参数和3D点位置。
- **仿射模糊性**：分解结果不唯一，需引入欧几里得约束（如图像轴垂直）来解决。

### 2.5 立体视觉（Stereo Vision）
- **基本原理**：模仿人类双眼视觉，利用两台相机拍摄的图像间的视差（Disparity）计算深度。深度公式为：
 $$
  z = \frac{f \cdot L}{d}
 $$
  其中$z$为深度，$f$为焦距，$L$为两相机间的基线距离，$d$为视差。
- **核心问题**：
  1. **对应问题**：在两幅图像中找到相同点的对应关系。
  2. **相机几何**：估计两相机之间的相对位置和方向。
  3. **场景几何**：从对应点和相机参数重建3D点位置。
- **对极几何（Epipolar Geometry）**：
  - **对极平面**：由两相机中心和3D点构成的平面。
  - **对极线**：3D点在另一幅图像中的投影约束线。
  - **对极点**：一相机中心在另一相机图像平面上的投影。
  - **基本矩阵（Fundamental Matrix）**：描述两幅图像间的对极约束，公式为$p'^T F p = 0$。
  - **本质矩阵（Essential Matrix）**：在标定相机下，描述两相机间的相对姿态，公式为$P'^T E P = 0$。
- **图像校正（Rectification）**：将两幅图像重新投影到一个共同平面，使对极线水平对齐，便于视差计算。
- **立体匹配（Stereo Matching）**：
  - **基本算法**：沿对极线搜索对应点，选择匹配代价最低的点。
  - **匹配方法**：包括SSD（平方差和）、符号特征匹配、SIFT特征等。
  - **深度学习方法**：如PSMNet（金字塔立体匹配网络）、GwcNet（分组相关立体网络）等，通过特征提取、成本体构建、成本聚合和视差回归实现高精度匹配。

---

## 第三部分：3D视觉的进阶技术

### 3.1 主动立体与结构光
- **主动立体**：通过投影结构光图案简化对应问题，常见于3D扫描和工业检测。
- **优点**：提高匹配精度，适合复杂表面。
- **局限性**：对环境光敏感，适用范围有限。

### 3.2 3D与深度学习的结合
- **单目深度估计**：从单张2D图像恢复深度信息，属于病态问题，依赖于线性透视、相对大小等线索。
- **多视角CNN**：通过多视角图像渲染，利用2D CNN提取特征并融合（如View Pooling），实现3D形状识别。
- **3D CNN**：直接在体数据（如体素网格）上进行3D卷积，计算复杂度高。
- **稀疏数据处理**：利用3D数据的稀疏性（如点云、八叉树），降低计算成本。
- **生成模型**：
  - **GAN for 3D**：生成3D体积或点云。
  - **从图像到表面**：如AtlasNet，通过学习将平面变形为3D表面。

### 3.3 3D数据应用
- **机器人**：用于环境感知和路径规划。
- **增强现实（AR）**：实现虚拟物体与现实场景的融合。
- **自动驾驶**：利用LiDAR和立体视觉进行障碍物检测。
- **医疗影像处理**：如青光眼检测中的立体眼底成像。

### 3.4 数据集
- **KITTI**：自动驾驶场景的LiDAR数据，包含3D边界框标注。
- **Semantic KITTI**：逐点标注的LiDAR数据。
- **Waymo Open Dataset**：大规模自动驾驶LiDAR数据。

---

## 第四部分：总结与学习目标

### 4.1 核心知识点回顾
- **3D视觉技术**：ToF、LiDAR、结构光、SfM、立体视觉。
- **关键算法**：关键点检测与跟踪、特征匹配、立体匹配。
- **数学基础**：透视投影、仿射投影、对极几何、基本/本质矩阵。
- **现代方法**：深度学习在3D重建和匹配中的应用。

### 4.2 学习目标
- 掌握3D视觉的基本原理和方法，理解各种技术的优缺点。
- 熟悉关键点检测、特征匹配和立体匹配的算法流程。
- 了解对极几何和基本矩阵在立体视觉中的作用。
- 探索深度学习在3D视觉中的应用，关注最新研究进展。

---

